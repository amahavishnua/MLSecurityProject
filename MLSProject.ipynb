{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLSProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fg_2AQveCE0a",
        "EQ_WWZwU7qUk",
        "wmIcYfT_G1Mr",
        "7x-NtsWDLJRX",
        "Z6PQyyC5Ihgp",
        "XB1UCqUhJRn6",
        "-H1MXO1iVUea"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg_2AQveCE0a"
      },
      "source": [
        "##IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD4i7XdQCvg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765fec43-ade0-448d-e32d-66de508d7390"
      },
      "source": [
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhgYiKCrNkZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1215749b-2088-4a9d-f36d-82af6c5bd07a"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import h5py\n",
        "import sys\n",
        "from decimal import Decimal\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import keras\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "from keras import backend as K\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.metrics import categorical_accuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import UpSampling2D, Cropping2D\n",
        "from keras import models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt5pi5roCJMe"
      },
      "source": [
        "## Running this Notebook \n",
        "https://drive.google.com/drive/folders/1mf9UHHPq6tg8kZGlFTrFpCJfkg4zhWiB?usp=sharing\n",
        "\n",
        "Can be open this with NYU account,\n",
        "\n",
        "Please add this to your google Drive and you're good to go\n",
        "\n",
        "You can find results you generated in \"results\" and \"repaired-nets\" folders\n",
        "We stored results we got in \"results_we_got\" and \"repaired-nets_we_got\"\n",
        "So if you open those you can directly verify the results we are able to obtain\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSQaHW4tFVsM"
      },
      "source": [
        "**GET DATA FROM DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IZgXflOOx46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a5703a5-529b-496a-e93a-f6705c059d81"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "sys.path.append(os.path.abspath(\"/content/drive/MyDrive/MLSecurityProject/\")) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQ_WWZwU7qUk"
      },
      "source": [
        "## API PARAMETERS\n",
        "\n",
        "Change the file names hear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP0-m45p7tVw"
      },
      "source": [
        "MODEL_DIR = '/content/drive/MyDrive/MLSecurityProject/models'  # model directory\n",
        "MODEL_FILENAME = 'sunglasses_bd_net.h5'  # badnet model file\n",
        "\n",
        "DATA_DIR = '/content/drive/MyDrive/MLSecurityProject/data'  # data folder\n",
        "DATA_FILE = 'clean_validation_data.h5' # dataset file\n",
        "POISONED_FILE = 'sunglasses_poisoned_data.h5'\n",
        "TEST_FILE = 'clean_test_data.h5'\n",
        "\n",
        "RESULT_DIR = '/content/drive/MyDrive/MLSecurityProject/results'  # directory for storing results\n",
        "# image filename template for visualization results\n",
        "IMG_FILENAME_TEMPLATE = 'visualize_%s_label_%d.png'\n",
        "REPAIRED_NETS= '/content/drive/MyDrive/MLSecurityProject/repaired-networks'\n",
        "REPAIRED_NET_FILENAME_TEMPLATE = 'repaired_net_%s.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmIcYfT_G1Mr"
      },
      "source": [
        "## DATA PROCESSING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_S-4dFJKqfQ"
      },
      "source": [
        "def save_image(x, file, frmt):\n",
        "    img = image.array_to_img(x, scale=False)\n",
        "    img.save(file, frmt)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp61rv1cKtFQ"
      },
      "source": [
        "def load_data(data_file, keys=None):\n",
        "    data = {}\n",
        "    with h5py.File(data_file, 'r') as hf:\n",
        "        if keys is None:\n",
        "            for n in hf:\n",
        "                data[n] = np.array(hf.get(n))\n",
        "        else:\n",
        "            for n in keys:\n",
        "                data[n] = np.array(hf.get(n))\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubNcmmA7-sAk"
      },
      "source": [
        "def unique_data(X_test, Y_test):\n",
        "    Y_test_unique = np.unique(Y_test)\n",
        "    return len(Y_test_unique)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fbVLy1yWOSW"
      },
      "source": [
        "def ld_data(data=('%s/%s' % (DATA_DIR, DATA_FILE))):\n",
        "    my_data = load_data(data, keys=['data', 'label'])\n",
        "    X = np.array(my_data['data'], dtype='float32')\n",
        "    Y = np.array(my_data['label'], dtype='float32')\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HXQogxDyTDz"
      },
      "source": [
        "def build_data_loader(X, Y):\n",
        "    img_gen = ImageDataGenerator().flow(X, Y, batch_size=batchSize)\n",
        "    return img_gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x-NtsWDLJRX"
      },
      "source": [
        "## VISUALISER CLASS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9xRY7dFLIZP"
      },
      "source": [
        "#### referred from https://github.com/bolunwang/backdoor\n",
        "class Visualizer:\n",
        "    \n",
        "    # type of regularization of the mask\n",
        "    REGULARIZATION = 'l1'\n",
        "    # threshold of attack success rate for dynamically changing cost\n",
        "    ATTACK_SUCC_THRESHOLD = 0.99\n",
        "    # patience\n",
        "    PATIENCE = 5\n",
        "    # multiple of changing cost, down multiple is the square root of this\n",
        "    COST_MULTIPLIER = 2 \n",
        "    # if resetting cost to 0 at the beginning\n",
        "    # default is true for full optimization, set to false for early detection\n",
        "    RESET_COST_TO_ZERO = True\n",
        "    # min/max of mask\n",
        "    MASK_MIN = 0\n",
        "    MASK_MAX = 1\n",
        "    # min/max of raw pixel intensity\n",
        "    COLOR_MIN = 0\n",
        "    COLOR_MAX = 255\n",
        "    # whether to shuffle during each epoch\n",
        "    SHUFFLE = True\n",
        "    # batch size of optimization\n",
        "    BATCH_SIZE = 32\n",
        "    # verbose level, 0, 1 or 2\n",
        "    VERBOSE = 1\n",
        "    # whether to return log or not\n",
        "    RETURN_LOGS = True\n",
        "    # whether to save last pattern or best pattern\n",
        "    SAVE_LAST = False\n",
        "    # epsilon used in tanh\n",
        "    EPSILON = K.epsilon()\n",
        "    # early stop flag\n",
        "    EARLY_STOP = True\n",
        "    # early stop threshold\n",
        "    EARLY_STOP_THRESHOLD = 0.99\n",
        "    # early stop patience\n",
        "    EARLY_STOP_PATIENCE = 5 * PATIENCE\n",
        "    # save tmp masks, for debugging purpose\n",
        "    SAVE_TMP = False\n",
        "    # dir to save intermediate masks\n",
        "    TMP_DIR = 'tmp'\n",
        "    # whether input image has been preprocessed or not\n",
        "    RAW_INPUT_FLAG = False\n",
        "\n",
        "    def __init__(self, model, regularization, input_shape, \n",
        "                 init_cost, steps, mini_batch, lr, num_classes,\n",
        "                 attack_succ_threshold=ATTACK_SUCC_THRESHOLD,\n",
        "                 patience=PATIENCE, cost_multiplier=COST_MULTIPLIER,\n",
        "                 reset_cost_to_zero=RESET_COST_TO_ZERO,\n",
        "                 mask_min=MASK_MIN, mask_max=MASK_MAX,\n",
        "                 color_min=COLOR_MIN, color_max=COLOR_MAX, \n",
        "                 shuffle=SHUFFLE, batch_size=BATCH_SIZE, verbose=VERBOSE,\n",
        "                 return_logs=RETURN_LOGS, save_last=SAVE_LAST,\n",
        "                 epsilon=EPSILON,\n",
        "                 early_stop=EARLY_STOP,\n",
        "                 early_stop_threshold=EARLY_STOP_THRESHOLD,\n",
        "                 early_stop_patience=EARLY_STOP_PATIENCE,\n",
        "                 save_tmp=SAVE_TMP, tmp_dir=TMP_DIR):\n",
        "\n",
        "        self.model = model\n",
        "        self.regularization = regularization\n",
        "        self.input_shape = input_shape\n",
        "        self.init_cost = init_cost\n",
        "        self.steps = steps\n",
        "        self.mini_batch = mini_batch\n",
        "        self.lr = lr\n",
        "        self.num_classes = totalClasses\n",
        "        self.attack_succ_threshold = attack_succ_threshold\n",
        "        self.patience = patience\n",
        "        self.cost_multiplier_up = cost_multiplier\n",
        "        self.cost_multiplier_down = cost_multiplier ** 1.5\n",
        "        self.reset_cost_to_zero = reset_cost_to_zero\n",
        "        self.mask_min = mask_min\n",
        "        self.mask_max = mask_max\n",
        "        self.color_min = color_min\n",
        "        self.color_max = color_max\n",
        "        self.shuffle = shuffle\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.return_logs = return_logs\n",
        "        self.save_last = save_last\n",
        "        self.epsilon = epsilon\n",
        "        self.early_stop = early_stop\n",
        "        self.early_stop_threshold = early_stop_threshold\n",
        "        self.early_stop_patience = early_stop_patience\n",
        "        self.save_tmp = save_tmp\n",
        "        self.tmp_dir = tmp_dir\n",
        "\n",
        "        mask_size =np.array(input_shape[0:2], dtype=float) \n",
        "        mask_size = mask_size.astype(int)\n",
        "        self.mask_size = mask_size\n",
        "        mask = np.zeros(self.mask_size)\n",
        "        pattern = np.zeros(input_shape)\n",
        "        mask = np.expand_dims(mask, axis=2)\n",
        "\n",
        "        mask_tanh = np.zeros_like(mask)\n",
        "        pattern_tanh = np.zeros_like(pattern)\n",
        "\n",
        "        # prepare mask related tensors\n",
        "        self.mask_tanh_tensor = K.variable(mask_tanh)\n",
        "        mask_tensor_unrepeat = (K.tanh(self.mask_tanh_tensor) /\n",
        "                                (2 - self.epsilon) +\n",
        "                                0.5)\n",
        "        mask_tensor_unexpand = K.repeat_elements(\n",
        "            mask_tensor_unrepeat,\n",
        "            rep=3, \n",
        "            axis=2)\n",
        "        self.mask_tensor = K.expand_dims(mask_tensor_unexpand, axis=0)\n",
        "        upsample_layer = UpSampling2D(size=(1, 1))\n",
        "        mask_upsample_tensor_uncrop = upsample_layer(self.mask_tensor)\n",
        "        uncrop_shape = K.int_shape(mask_upsample_tensor_uncrop)[1:]\n",
        "        cropping_layer = Cropping2D(\n",
        "            cropping=((0, uncrop_shape[0] - self.input_shape[0]),\n",
        "                      (0, uncrop_shape[1] - self.input_shape[1])))\n",
        "        self.mask_upsample_tensor = cropping_layer(\n",
        "            mask_upsample_tensor_uncrop)\n",
        "        reverse_mask_tensor = (K.ones_like(self.mask_upsample_tensor) -\n",
        "                               self.mask_upsample_tensor)\n",
        "\n",
        "        # prepare pattern related tensors\n",
        "        self.pattern_tanh_tensor = K.variable(pattern_tanh)\n",
        "        self.pattern_raw_tensor = (\n",
        "            (K.tanh(self.pattern_tanh_tensor) / (2 - self.epsilon) + 0.5) *\n",
        "            255.0)\n",
        "\n",
        "        # prepare input image related tensors\n",
        "        # ignore clip operation here\n",
        "        # assume input image is already clipped into valid color range\n",
        "        input_tensor = K.placeholder(model.input_shape)\n",
        "        input_raw_tensor = input_tensor * 255.0\n",
        "\n",
        "        # IMPORTANT: MASK OPERATION IN RAW DOMAIN\n",
        "        X_adv_raw_tensor = (\n",
        "            reverse_mask_tensor * input_raw_tensor +\n",
        "            self.mask_upsample_tensor * self.pattern_raw_tensor)\n",
        "\n",
        "        X_adv_tensor = X_adv_raw_tensor/255.0\n",
        "\n",
        "        output_tensor = model(X_adv_tensor)\n",
        "        y_true_tensor = K.placeholder(model.output_shape)\n",
        "\n",
        "        self.loss_acc = categorical_accuracy(output_tensor, y_true_tensor)\n",
        "\n",
        "        self.loss_ce = categorical_crossentropy(output_tensor, y_true_tensor)\n",
        "\n",
        "        if self.regularization is None:\n",
        "            self.loss_reg = K.constant(0)\n",
        "        elif self.regularization is 'l1':\n",
        "            self.loss_reg = (K.sum(K.abs(self.mask_upsample_tensor)) /3 )\n",
        "\n",
        "        cost = self.init_cost\n",
        "        self.cost_tensor = K.variable(cost)\n",
        "        self.loss = self.loss_ce + self.loss_reg * self.cost_tensor\n",
        "\n",
        "        self.opt = Adam(lr=self.lr, beta_1=0.5, beta_2=0.9)\n",
        "        self.updates = self.opt.get_updates(\n",
        "            params=[self.pattern_tanh_tensor, self.mask_tanh_tensor],\n",
        "            loss=self.loss)\n",
        "        self.train = K.function(\n",
        "            [input_tensor, y_true_tensor],\n",
        "            [self.loss_ce, self.loss_reg, self.loss, self.loss_acc],\n",
        "            updates=self.updates)\n",
        "\n",
        "\n",
        "    def reset_opt(self):\n",
        "\n",
        "        K.set_value(self.opt.iterations, 0)\n",
        "        for w in self.opt.weights:\n",
        "            K.set_value(w, np.zeros(K.int_shape(w)))\n",
        "\n",
        "    def reset_state(self, pattern_init, mask_init):\n",
        "\n",
        "        print('resetting state')\n",
        "\n",
        "        # setting cost\n",
        "        if self.reset_cost_to_zero:\n",
        "            self.cost = 0\n",
        "        else:\n",
        "            self.cost = self.init_cost\n",
        "        K.set_value(self.cost_tensor, self.cost)\n",
        "\n",
        "        # setting mask and pattern\n",
        "        mask = np.array(mask_init)\n",
        "        pattern = np.array(pattern_init)\n",
        "        mask = np.clip(mask, self.mask_min, self.mask_max)\n",
        "        pattern = np.clip(pattern, self.color_min, self.color_max)\n",
        "        mask = np.expand_dims(mask, axis=2)\n",
        "\n",
        "        # convert to tanh space\n",
        "        mask_tanh = np.arctanh((mask - 0.5) * (2 - self.epsilon))\n",
        "        pattern_tanh = np.arctanh((pattern / 255.0 - 0.5) * (2 - self.epsilon))\n",
        "        print('mask_tanh', np.min(mask_tanh), np.max(mask_tanh))\n",
        "        print('pattern_tanh', np.min(pattern_tanh), np.max(pattern_tanh))\n",
        "\n",
        "        K.set_value(self.mask_tanh_tensor, mask_tanh)\n",
        "        K.set_value(self.pattern_tanh_tensor, pattern_tanh)\n",
        "\n",
        "        # resetting optimizer states\n",
        "        self.reset_opt()\n",
        "\n",
        "\n",
        "    def save_tmp_func(self, step):\n",
        "\n",
        "        cur_mask = K.eval(self.mask_upsample_tensor)\n",
        "        cur_mask = cur_mask[0, ..., 0]\n",
        "        img_filename = ('%s/%s' % (self.tmp_dir, 'tmp_mask_step_%d.png' % step))\n",
        "        save_image(np.expand_dims(cur_mask, axis=2) * 255,\n",
        "                                  img_filename,\n",
        "                                  'png')\n",
        "\n",
        "        cur_fusion = K.eval(self.mask_upsample_tensor *\n",
        "                            self.pattern_raw_tensor)\n",
        "        cur_fusion = cur_fusion[0, ...]\n",
        "        img_filename = ('%s/%s' % (self.tmp_dir, 'tmp_fusion_step_%d.png' % step))\n",
        "        save_image(cur_fusion, img_filename, 'png')\n",
        "\n",
        "\n",
        "    def visualize(self, gen, y, pattern_init, mask_init):\n",
        "\n",
        "        # since we use a single optimizer repeatedly, we need to reset\n",
        "        # optimzier's internal states before running the optimization\n",
        "        self.reset_state(pattern_init, mask_init)\n",
        "\n",
        "        # best optimization results\n",
        "        mask_best = None\n",
        "        mask_upsample_best = None\n",
        "        pattern_best = None\n",
        "        reg_best = float('inf')\n",
        "\n",
        "        # logs and counters for adjusting balance cost\n",
        "        logs = []\n",
        "        cost_set_counter = 0\n",
        "        cost_up_counter = 0\n",
        "        cost_down_counter = 0\n",
        "        cost_up_flag = False\n",
        "        cost_down_flag = False\n",
        "\n",
        "        # counter for early stop\n",
        "        early_stop_counter = 0\n",
        "        early_stop_reg_best = reg_best\n",
        "\n",
        "        # vectorized target\n",
        "        Y_ = to_categorical([y]* self.batch_size ,self.num_classes) \n",
        "\n",
        "        # loop start\n",
        "        for step in range(self.steps):\n",
        "\n",
        "            # record loss for all mini-batches\n",
        "            loss_ce_list = []\n",
        "            loss_reg_list = []\n",
        "            loss_list = []\n",
        "            loss_acc_list = []\n",
        "            for idx in range(self.mini_batch):\n",
        "                X_batch, _ = gen.next()\n",
        "                if X_batch.shape[0] != Y_.shape[0]:\n",
        "                    Y_ = to_categorical([y]* X_batch.shape[0],self.num_classes)\n",
        "                (loss_ce_value,\n",
        "                    loss_reg_value,\n",
        "                    loss_value,\n",
        "                    loss_acc_value) = self.train([X_batch, Y_])\n",
        "                loss_ce_list.extend(list(loss_ce_value.flatten()))\n",
        "                loss_reg_list.extend(list(loss_reg_value.flatten()))\n",
        "                loss_list.extend(list(loss_value.flatten()))\n",
        "                loss_acc_list.extend(list(loss_acc_value.flatten()))\n",
        "\n",
        "            avg_loss_ce = np.mean(loss_ce_list)\n",
        "            avg_loss_reg = np.mean(loss_reg_list)\n",
        "            avg_loss = np.mean(loss_list)\n",
        "            avg_loss_acc = np.mean(loss_acc_list)\n",
        "\n",
        "            # check to save best mask or not\n",
        "            if avg_loss_acc >= self.attack_succ_threshold and avg_loss_reg < reg_best:\n",
        "                mask_best = K.eval(self.mask_tensor)\n",
        "                mask_best = mask_best[0, ..., 0]\n",
        "                mask_upsample_best = K.eval(self.mask_upsample_tensor)\n",
        "                mask_upsample_best = mask_upsample_best[0, ..., 0]\n",
        "                pattern_best = K.eval(self.pattern_raw_tensor)\n",
        "                reg_best = avg_loss_reg\n",
        "\n",
        "            # verbose\n",
        "            if self.verbose != 0:\n",
        "                if self.verbose == 2 or step % (self.steps // 10) == 0:\n",
        "                    print('step: %3d, cost: %.2E, attack: %.3f, loss: %f, ce: %f, reg: %f, reg_best: %f' %\n",
        "                          (step, Decimal(self.cost), avg_loss_acc, avg_loss,\n",
        "                           avg_loss_ce, avg_loss_reg, reg_best))\n",
        "\n",
        "            # save log\n",
        "            logs.append((step,\n",
        "                         avg_loss_ce, avg_loss_reg, avg_loss, avg_loss_acc,\n",
        "                         reg_best, self.cost))\n",
        "\n",
        "            # check early stop\n",
        "            if self.early_stop:\n",
        "                # only terminate if a valid attack has been found\n",
        "                if reg_best < float('inf'):\n",
        "                    if reg_best >= self.early_stop_threshold * early_stop_reg_best:\n",
        "                        early_stop_counter += 1\n",
        "                    else:\n",
        "                        early_stop_counter = 0\n",
        "                early_stop_reg_best = min(reg_best, early_stop_reg_best)\n",
        "\n",
        "                if (cost_down_flag and\n",
        "                        cost_up_flag and\n",
        "                        early_stop_counter >= self.early_stop_patience):\n",
        "                    print('early stop')\n",
        "                    break\n",
        "\n",
        "            # check cost modification\n",
        "            if self.cost == 0 and avg_loss_acc >= self.attack_succ_threshold:\n",
        "                cost_set_counter += 1\n",
        "                if cost_set_counter >= self.patience:\n",
        "                    self.cost = self.init_cost\n",
        "                    K.set_value(self.cost_tensor, self.cost)\n",
        "                    cost_up_counter = 0\n",
        "                    cost_down_counter = 0\n",
        "                    cost_up_flag = False\n",
        "                    cost_down_flag = False\n",
        "                    print('initialize cost to %.2E' % Decimal(self.cost))\n",
        "            else:\n",
        "                cost_set_counter = 0\n",
        "\n",
        "            if avg_loss_acc >= self.attack_succ_threshold:\n",
        "                cost_up_counter += 1\n",
        "                cost_down_counter = 0\n",
        "            else:\n",
        "                cost_up_counter = 0\n",
        "                cost_down_counter += 1\n",
        "\n",
        "            if cost_up_counter >= self.patience:\n",
        "                cost_up_counter = 0\n",
        "                if self.verbose == 2:\n",
        "                    print('up cost from %.2E to %.2E' %\n",
        "                          (Decimal(self.cost),\n",
        "                           Decimal(self.cost * self.cost_multiplier_up)))\n",
        "                self.cost *= self.cost_multiplier_up\n",
        "                K.set_value(self.cost_tensor, self.cost)\n",
        "                cost_up_flag = True\n",
        "            elif cost_down_counter >= self.patience:\n",
        "                cost_down_counter = 0\n",
        "                if self.verbose == 2:\n",
        "                    print('down cost from %.2E to %.2E' %\n",
        "                          (Decimal(self.cost),\n",
        "                           Decimal(self.cost / self.cost_multiplier_down)))\n",
        "                self.cost /= self.cost_multiplier_down\n",
        "                K.set_value(self.cost_tensor, self.cost)\n",
        "                cost_down_flag = True\n",
        "\n",
        "            if self.save_tmp:\n",
        "                self.save_tmp_func(step)\n",
        "\n",
        "        # save the final version\n",
        "        if mask_best is None or self.save_last:\n",
        "            mask_best = K.eval(self.mask_tensor)\n",
        "            mask_best = mask_best[0, ..., 0]\n",
        "            mask_upsample_best = K.eval(self.mask_upsample_tensor)\n",
        "            mask_upsample_best = mask_upsample_best[0, ..., 0]\n",
        "            pattern_best = K.eval(self.pattern_raw_tensor)\n",
        "\n",
        "        if self.return_logs:\n",
        "            return pattern_best, mask_best, mask_upsample_best, logs\n",
        "        else:\n",
        "            return pattern_best, mask_best, mask_upsample_best"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6PQyyC5Ihgp"
      },
      "source": [
        "## GET POTENTIAL TRIGGERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msq9JafkWQvY"
      },
      "source": [
        "def trigger_visualizer(visualizer, gen, y, pattern_flag=True):\n",
        "    myPattern = np.random.random(inpShape) * 255.0\n",
        "    myMask = np.random.random(maskShape)\n",
        "    finalPattern, finalMask, finalMaskUpsample, logs = visualizer.visualize(\n",
        "        gen=gen, y=y, pattern_init=myPattern, mask_init=myMask)\n",
        "    if pattern_flag:\n",
        "        save_pattern(finalPattern, finalMaskUpsample,y)\n",
        "\n",
        "    return finalPattern, finalMaskUpsample, logs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epDDsPiRWbGV"
      },
      "source": [
        "def save_pattern(pattern, mask, y):\n",
        "    if not os.path.exists(RESULT_DIR):\n",
        "        os.mkdir(RESULT_DIR)\n",
        "\n",
        "    img_filename = ('%s/%s' % (RESULT_DIR,IMG_FILENAME_TEMPLATE % ('pattern', y)))\n",
        "    save_image(pattern, img_filename, 'png')\n",
        "\n",
        "    img_filename = ('%s/%s' % (RESULT_DIR,IMG_FILENAME_TEMPLATE % ('mask', y)))\n",
        "    save_image(np.expand_dims(mask, axis=2) * 255,img_filename,'png')\n",
        "\n",
        "    fusion = np.multiply(pattern, np.expand_dims(mask, axis=2))\n",
        "    img_filename = ('%s/%s' % (RESULT_DIR,IMG_FILENAME_TEMPLATE % ('fusion', y)))\n",
        "    save_image(fusion, img_filename, 'png')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLkVl4rqWf79"
      },
      "source": [
        "def get_potential_triggers():\n",
        "    X_test, Y_test = ld_data()\n",
        "    X_test=X_test.reshape(X_test.shape[0],X_test.shape[2],X_test.shape[3],X_test.shape[1])\n",
        "    test_generator = build_data_loader(X_test, Y_test,)\n",
        "    model_file = '%s/%s' % (MODEL_DIR, MODEL_FILENAME)\n",
        "    model = load_model(model_file)\n",
        "    visualizer_pi = Visualizer(model,regularization='l1',input_shape=inpShape,init_cost=initCost, steps=numSteps, lr=lr, num_classes=totalClasses,mini_batch=min_batch )\n",
        "    logs_ = {}\n",
        "    class_list = list(range(totalClasses))\n",
        "    for cl in range(1241,totalClasses):\n",
        "\n",
        "        print('processing label %d' % cl)\n",
        "\n",
        "        pattern_best, _, logs = trigger_visualizer(\n",
        "            visualizer_pi, test_generator, y=cl,\n",
        "            pattern_flag=True)\n",
        "        logs_[cl] = logs\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB1UCqUhJRn6"
      },
      "source": [
        "## FILTER POTENTIAL TRIGGERS TO GET ACTUAL TRIGGERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1csaGTfdLd3"
      },
      "source": [
        "def detect_outliers(normList, idx):\n",
        "    consistConst = 1.4826  \n",
        "    median = np.median(normList)\n",
        "    mad = consistConst * np.median(np.abs(normList - median))\n",
        "    minMad = np.abs(np.min(normList) - median)\n",
        "    minMad = minMad/mad\n",
        "\n",
        "    print('median: %f, MAD: %f' % (median, mad))\n",
        "    print('anomaly index: %f' % minMad)\n",
        "\n",
        "    flagList = []\n",
        "    for cl in idx:\n",
        "        if normList[idx[cl]] < median and np.abs(normList[idx[cl]] - median) / mad > 2:\n",
        "            flagList.append((cl, normList[idx[cl]]))\n",
        "\n",
        "    if len(flagList) > 0:\n",
        "        flagList = sorted(flagList, key=lambda x: x[1])\n",
        "\n",
        "    print('flagged list: %s' %', '.join(['%d: %2f' % (cl, norm)for cl, norm in flagList]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPSd8U2dJte6"
      },
      "source": [
        "def analyze_pattern():\n",
        "    maskFlatten = []\n",
        "    idx = {}\n",
        "    for y in range(totalClasses):\n",
        "        maskFile = IMG_FILENAME_TEMPLATE % ('mask', y)\n",
        "        if os.path.isfile('%s/%s' % (RESULT_DIR, maskFile)):\n",
        "            img = image.load_img('%s/%s' % (RESULT_DIR, maskFile),color_mode='grayscale',target_size=inpShape)\n",
        "            mask = image.img_to_array(img)\n",
        "            mask = mask/255\n",
        "            mask = mask[:, :, 0]\n",
        "\n",
        "            maskFlatten.append(mask.flatten())\n",
        "\n",
        "            idx[y] = len(maskFlatten) - 1\n",
        "\n",
        "    normList = [np.sum(np.abs(m)) for m in maskFlatten]\n",
        "\n",
        "    print('%d labels found' % len(normList))\n",
        "\n",
        "    detect_outliers(normList, idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gT5bNhdYsR04"
      },
      "source": [
        "## Backdoor mitigation functions <br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw6dYSGVsQzp"
      },
      "source": [
        "def get_random_samples(X, Y, split):\n",
        "    X_tr, X_clean,y_tr, y_clean= train_test_split(X, Y, train_size=split, random_state=42, shuffle=True)\n",
        "    return X_tr,X_clean,y_tr, y_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7omcMRARD09"
      },
      "source": [
        "def get_mask_pattern(y):\n",
        "    maskFile = IMG_FILENAME_TEMPLATE % ('mask', y)\n",
        "    mask = None\n",
        "    pattern = None\n",
        "    img = image.load_img('%s/%s' % (RESULT_DIR, maskFile),target_size=inpShape)\n",
        "    mask = image.img_to_array(img)\n",
        "\n",
        "    patternFile = IMG_FILENAME_TEMPLATE % ('pattern', y)\n",
        "    img = image.load_img('%s/%s' % (RESULT_DIR, patternFile),target_size=inpShape)\n",
        "    pattern = image.img_to_array(img)\n",
        "    return mask, pattern\n",
        "\n",
        "def injection(mask, pattern, adv_img):\n",
        "    return mask * pattern + (1 - mask) * adv_img\n",
        "\n",
        "def infect_X(img, tgt):\n",
        "    mask, pattern = get_mask_pattern(tgt)\n",
        "    rawImg = np.copy(img)\n",
        "    advImg = np.copy(rawImg)\n",
        "    advImg = injection(mask, pattern, advImg)\n",
        "    return advImg, tgt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSB9j0YyhEps"
      },
      "source": [
        "def get_avg_neuron_activation():\n",
        "    modelFile = '%s/%s' % (MODEL_DIR, MODEL_FILENAME)\n",
        "    model_cl = load_model(modelFile)\n",
        "    model_cl.fit(x=X_mtg, y=Y_mtg, epochs=5 ,shuffle=True)\n",
        "    layer_outputs_cl = [layer.output for layer in model_cl.layers[12:13]] \n",
        "    activation_model_cl = models.Model(inputs=model_cl.input, outputs=layer_outputs_cl)\n",
        "\n",
        "    model_ps = load_model(modelFile)\n",
        "    model_ps.fit(x= patchImages, y=patchLabels, epochs=5 ,shuffle=True)\n",
        "    layer_outputs_ps = [layer.output for layer in model_ps.layers[12:13]] \n",
        "    activation_model_ps = models.Model(inputs=model_ps.input, outputs=layer_outputs_ps)\n",
        "\n",
        "    activations_cl=[]\n",
        "    activations_cl=activation_model_cl.predict(X_mtg[:1000])\n",
        "    avg_activation_cl = sum(np.array(activations_cl))/len(activations_cl) \n",
        "\n",
        "    activations_ps=[]\n",
        "    activations_ps =activation_model_ps.predict(patchImages[:1000])\n",
        "    avg_activation_ps = sum(np.array(activations_ps))/len(activations_ps) \n",
        "    diff_activation = avg_activation_ps - avg_activation_cl\n",
        "    mean_act = sum(avg_activation_cl)/len(avg_activation_cl)\n",
        "    return mean_act,diff_activation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKtOwZ5DanA8"
      },
      "source": [
        "#Filter for Detecting Adversarial Inputs using Nueron Activations\n",
        "\n",
        "def detect_adversial_inputs(model,x_i,mean_act):\n",
        "    x=[]\n",
        "    x.append(x_i)\n",
        "    x=np.array(x)\n",
        "    layer_outputs = [layer.output for layer in model.layers[12:13]] \n",
        "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "    for neuron in activation_model.predict(x)[0] :\n",
        "        if neuron > mean_act+1000 :\n",
        "          return True\n",
        "    return False\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaoIcbnAdqVr"
      },
      "source": [
        "#Neuron pruning by setting the neuron value to zero in last but 2nd layer when weight of the neuron > threshold\n",
        "def neuron_pruning(model):\n",
        "    modelFile = '%s/%s' % (MODEL_DIR, MODEL_FILENAME)\n",
        "    new_model = load_model(modelFile)\n",
        "    new_weights = model.get_weights().copy()\n",
        "    for layer in new_weights[12]:\n",
        "      for i in range(0,len(layer)):\n",
        "          if layer[i] > mean_act+1000 :\n",
        "              layer[i] =0\n",
        "    return new_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpiMSJJZAIxX"
      },
      "source": [
        "def reverse_trigger_to_clean_images(X_mtg, Y_mtg):\n",
        "    patchImages = []\n",
        "    patchLabels = []\n",
        "    imgIndex=[]\n",
        "    for ind, img in enumerate(X_mtg):\n",
        "        y = Y_mtg[ind]\n",
        "        patchImg, patchLabel = infect_X(img, y)\n",
        "        patchImages.append(patchImg)\n",
        "        patchLabels.append(patchLabel)\n",
        "        imgIndex.append(ind)\n",
        "\n",
        "    patchImages = np.copy(patchImages)\n",
        "    patchLabels = np.copy(patchLabels)\n",
        "    return patchImages, patchLabels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-H1MXO1iVUea"
      },
      "source": [
        "##PARAMETERS \n",
        "\n",
        "set the optimization parameters here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fmxM-SEVA1K"
      },
      "source": [
        "X, Y = ld_data()\n",
        "xx=X.reshape(X.shape[0],X.shape[2],X.shape[3],X.shape[1])\n",
        "inpShape = (xx.shape[1], xx.shape[2], xx.shape[3])\n",
        "totalClasses = unique_data(X, Y) \n",
        "# parameters for optimization\n",
        "batchSize= 10\n",
        "lr = 0.1  # learning rate\n",
        "numSteps = 100  # total optimization iterations \n",
        "num_sample_mb = 100  # number of samples in each mini batch\n",
        "min_batch = num_sample_mb // batchSize  # mini batch size used for early stop\n",
        "initCost = 1e-3  # initial weight used for balancing two objectives\n",
        "maskShape = np.ceil(np.array(inpShape[0:2], dtype=float) )\n",
        "maskShape = maskShape.astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzZzirbNLxn2"
      },
      "source": [
        "## API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCYK8eMQOhjN"
      },
      "source": [
        "repairedNetworkFile =REPAIRED_NETS+'/'+REPAIRED_NET_FILENAME_TEMPLATE % MODEL_FILENAME.split('.')[0]\n",
        "X_tr,X_mtg,Y_tr,Y_mtg = get_random_samples(xx, Y, 0.7)\n",
        "patchImages, patchLabels = reverse_trigger_to_clean_images(X_mtg, Y_mtg)\n",
        "mean_act,_ = get_avg_neuron_activation()\n",
        "def repaired_network():\n",
        "    get_potential_triggers()\n",
        "    analyze_pattern()\n",
        "\n",
        "    #Neuron Pruning over the model\n",
        "    modelFile = '%s/%s' % (MODEL_DIR, MODEL_FILENAME)\n",
        "    model = load_model(modelFile)\n",
        "    model.fit(x=X_tr, y=Y_tr, epochs=10 ,shuffle=True)\n",
        "    model=neuron_pruning(model)\n",
        "    model.save(repairedNetworkFile)\n",
        "    model=load_model(repairedNetworkFile)\n",
        "\n",
        "    # Evaluation over Neuron Pruned model with clean data\n",
        "    X_test,Y_test=ld_data(data=('%s/%s' % (DATA_DIR, TEST_FILE)))\n",
        "    X_test=X_test.reshape(X_test.shape[0],X_test.shape[2],X_test.shape[3],X_test.shape[1])\n",
        "    _,accuracy = model.evaluate(x=X_test, y=Y_test)\n",
        "    print(\"accuracy on clean data:\"+ str(accuracy))\n",
        "\n",
        "    # Evaluation over Neuron Pruned model with poisonous data\n",
        "    X_ps , Y_ps =ld_data(data=('%s/%s' % (DATA_DIR, POISONED_FILE)))\n",
        "    X_ps=X_ps.reshape(X_ps.shape[0],X_ps.shape[2],X_ps.shape[3],X_ps.shape[1])\n",
        "    _,accuracy = model.evaluate(x=X_test, y=Y_test)\n",
        "    print(\"accuracy on poisoned data:\"+ str(accuracy))\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1iM-VIYPLQ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dcb232-f733-47d0-eaa6-4858610e8e05"
      },
      "source": [
        "model =repaired_network()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12830/12830 [==============================] - 15s 1ms/step\n",
            "accuracy on clean data:0.00046765393926762044\n",
            "12830/12830 [==============================] - 14s 1ms/step\n",
            "accuracy on poisoned data:0.00046765393926762044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsSBKLcR1kHO"
      },
      "source": [
        "X_ps , Y_ps =ld_data(data=('%s/%s' % (DATA_DIR, POISONED_FILE)))\n",
        "X_ps=X_ps.reshape(X_ps.shape[0],X_ps.shape[2],X_ps.shape[3],X_ps.shape[1])\n",
        "preds=[]\n",
        "i=0\n",
        "for x in X_ps:\n",
        "    print(i)\n",
        "    i=i+1\n",
        "    #If poisoned set output variable as N+1th class\n",
        "    if detect_adversial_inputs(model,x,mean_act):\n",
        "         preds.append(totalClasses)\n",
        "    else:\n",
        "         p=model.predict(x)\n",
        "         preds.append(p)\n",
        "accuracy = np.mean(np.equal(preds, Y_ps))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}