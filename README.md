# Defense System aganist backdoor attacks on DNN
backdoor detector for BadNets trained on the YouTube Face dataset

## Introduction

The project is about detecting the backdoor attacks via input filters, neuron pruning and unlearning. So with the trained DNN model we have to find if there is any input trigger that would produce misleading classifications when trigger is added to input i.e(adversarial images)

## What is this backdoor attack ?

To know this first we have to know what doesn't fall into this category, 

* It is not image specific modification (not Adversarial attack)
* It isn't adversarial poisoning (where an incorrect label assosiation is done at training time or modifications on a trained model)

Thie Backdoor attack is where unexpected results will happen when a trigger is added to input. So if there is no trigger then this model is perfectly fine.

Bad Net: generated by training the model with the adversarial images and actual images, which gives 99% success rate. One other approach is Troajan Attack (latest one) is far more efficient and requires less data.

## Now Defense System aganist Backdoors
